{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to Grips with JSTOR data\n",
    "\n",
    "There is a lot of data in the zip files provided by JSTOR. How to get at it properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle as p\n",
    "from math import inf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build input pipeline for Gensim\n",
    "\n",
    "Gensim allows you to stream data when, as in this case, you might have too much to fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSTORCorpus(object):\n",
    "    \"\"\"Iterator for streaming articles from JSTOR DfR corpus into Gensim\"\"\"\n",
    "    \n",
    "    # For cleaning txt files. Finds xml tags or end-of-line hyphens to delete\n",
    "    CLEAN_RGX = re.compile('<[^>]+>|(?\\w)-\\s')\n",
    "    \n",
    "    def __init__(self, meta_dir, data_dir, corpus_meta=None):\n",
    "        self.meta_dir = meta_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.corpus_meta = corpus_meta\n",
    "        self.doc_types = set()\n",
    "        \n",
    "        # Ingest corpus if no existing corpus provided\n",
    "        if self.corpus_meta is None:\n",
    "            self.extract_jstor_meta(self.meta_dir, self.data_dir)\n",
    "        else:\n",
    "            # Otherwise loop over the corpus and extract doc_type information\n",
    "            self.doc_types = set([doc['type'] for key,doc in self.corpus_meta.items()])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for key in self.corpus_meta:\n",
    "            with open(key) as file:\n",
    "                # Get text\n",
    "                raw_xml = file.read()\n",
    "                # Strip tags\n",
    "                text = self.CLEAN_RGX.sub('', raw_xml)\n",
    "                # Yield array of tokens\n",
    "                yield wordpunct_tokenize(text)\n",
    "    \n",
    "    def extract_jstor_meta(self, meta_dir, data_dir):\n",
    "        \"\"\"Loop over directory of JSTOR metadata files, extracts key info from xml\n",
    "\n",
    "        Arguments:\n",
    "        meta_dir (str): directory where metadata files are held\n",
    "        data_dir (str): directory where data files are held\n",
    "        \"\"\"\n",
    "\n",
    "        self.corpus_meta = {}\n",
    "        \n",
    "        parsed = 0\n",
    "        skipped = 0\n",
    "\n",
    "        print(f'Parsing xml files in {meta_dir}. Associated .txt in {data_dir}')\n",
    "        \n",
    "        # The metadata file contains many documents without a text file. We don't want that!\n",
    "        actual_docs = set(os.listdir(data_dir))\n",
    "\n",
    "        for name in tqdm(os.listdir(meta_dir)):\n",
    "            \n",
    "            # Infer name of data file and check\n",
    "            txt_file = name[:-3] + 'txt' # replace .xml with .txt\n",
    "            if txt_file not in actual_docs:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Get doi (for book metadata)\n",
    "            doi = re.sub('^.+_', '', name[:-4])\n",
    "\n",
    "            # Locate data file\n",
    "            data_file = os.path.join(data_dir, txt_file) # fill path\n",
    "            \n",
    "            # Read in metadata file\n",
    "            with open(os.path.join(meta_dir, name)) as file:\n",
    "                meta_xml = BeautifulSoup(file.read())\n",
    "\n",
    "            # Get key metadata\n",
    "            doc_dict = {}\n",
    "\n",
    "            # For articles:\n",
    "            if name.startswith('journal-article'):\n",
    "                doc_dict['type'] = meta_xml.html.body.article['article-type']\n",
    "                # Store doc type in corpus metadata\n",
    "                self.doc_types.add(doc_dict['type'])\n",
    "                title = meta_xml.find(['article-title','source'])\n",
    "                if title is not None:\n",
    "                    doc_dict['title'] = title.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "\n",
    "            # For book chapters:\n",
    "            elif name.startswith('book-chapter'):\n",
    "                doc_dict['type'] = 'book-chapter'\n",
    "                self.doc_types.add('book-chapter')\n",
    "                # First book-id element is id of whole book\n",
    "                part_of = meta_xml.find('book-id')\n",
    "                if part_of is not None:\n",
    "                    doc_dict['part-of'] = part_of.get_text()\n",
    "                year = meta_xml.find('year')\n",
    "                if year is not None:\n",
    "                    doc_dict['year'] = year.get_text()\n",
    "                # Getting chapter title is slightly harder, because sometimes each book-part is labelled\n",
    "                # simply with the internal id, and sometimes with the doi\n",
    "                book_id = re.sub('.+_', '', doi)\n",
    "                book_rgx = re.compile(re.escape(book_id))\n",
    "                doc_dict['title'] = meta_xml.find('book-part-id', string=book_rgx).parent.find('title').get_text()\n",
    "\n",
    "            # Store in corpus_meta dict\n",
    "            self.corpus_meta[data_file] = doc_dict\n",
    "            \n",
    "            # Increment counter\n",
    "            parsed += 1\n",
    "\n",
    "        # Success message\n",
    "        print(f'{parsed} documents parsed successfully. {skipped} documents skipped.')\n",
    "        \n",
    "    def filter_by_year(self, min_year=1750, max_year=inf):\n",
    "        \"\"\"Filter the corpus according to minimum and maximum years\n",
    "        \n",
    "        Arguments:\n",
    "        min_year (int)\n",
    "        max_year (int)\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        orig_len = len(self.corpus_meta)\n",
    "        print(f'Filtering {orig_len} documents between years {min_year} and {max_year}...')\n",
    "        \n",
    "        for key,val_dict in self.corpus_meta.items():\n",
    "            # Skip files that cannot be parsed\n",
    "            if 'year' not in val_dict:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(val_dict['year'])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            # Apply conditions\n",
    "            if year <= max_year and year >= min_year:\n",
    "                filtered_corpus[key] = val_dict\n",
    "        \n",
    "        self.corpus_meta = filtered_corpus\n",
    "        \n",
    "        print(f'Corpus filtered. {orig_len - len(self.corpus_meta)} documents removed.')\n",
    "        \n",
    "    def filter_by_type(self, allowed_types):\n",
    "        \"\"\"Filter the corpus by doctype.\n",
    "        \n",
    "        Arguments:\n",
    "        allowed_types (list): a list of strings with the allowed doc_types\"\"\"\n",
    "        \n",
    "        filtered_corpus = {}\n",
    "        \n",
    "        orig_len = len(self.corpus_meta)\n",
    "        print(f'Filtering {orig_len} documents ...')\n",
    "        \n",
    "        for key, val_dict in self.corpus_meta.items():\n",
    "            if val_dict.type in allowed_types:\n",
    "                filtered_corpus[key] = val_dict\n",
    "                \n",
    "        self.corpus_meta = filtered_corpus\n",
    "        \n",
    "        print(f'Corpus filtered. {orig_len - len(self.corpus_meta)} documents removed.')\n",
    "        \n",
    "    def save(self, path=None):\n",
    "        \"\"\"Pickle the corpus metadata for later use.\n",
    "        \n",
    "        Arguments:\n",
    "        path (str): path to the save file\"\"\"\n",
    "        \n",
    "        if path is None:\n",
    "            path = time.strftime(\"%Y%m%d-%H%M%S\") + '-jstor-corpus.p'\n",
    "        \n",
    "        out = {'meta_dir':self.meta_dir, 'data_dir':self.data_dir, 'corpus_meta':self.corpus_meta}\n",
    "        \n",
    "        with open(path, 'wb') as file:\n",
    "            p.dump(out, file)\n",
    "        \n",
    "        print(f'Corpus saved to {path}')\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load a pickled corpus created by JSTORCorpus.save()\n",
    "    \n",
    "        Arguments:\n",
    "        path (str): path to the corpus\"\"\"\n",
    "\n",
    "        with open(path, 'rb') as corpus_file:\n",
    "            corpus = cls(**p.load(corpus_file))\n",
    "        \n",
    "        print(f'Corpus loaded from {path}')\n",
    "\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_dir = '../data/metadata'\n",
    "# data_dir = '../data/ocr'\n",
    "# corpus = JSTORCorpus(meta_dir=meta_dir, data_dir=data_dir)\n",
    "# corpus.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus loaded from data/last-15-years-corpus.p\n"
     ]
    }
   ],
   "source": [
    "corpus = JSTORCorpus.load('data/last-15-years-corpus.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test input pipeline with Gensim\n",
    "\n",
    "Gensim provides several methods for modelling a corpus as a whole. Let's try FastText and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Romantic is this corpus?\n",
    "\n",
    "The corpus was constructed by searching for the 'Romanticism' keyword. But it seems there are lots of articles here that aren't actually about Romanticism..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = []\n",
    "for vec in corpus:\n",
    "    count = len([word for word in vec if word.lower().startswith('romantic')])\n",
    "    total = len(vec)\n",
    "    freqs.append(count/total*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(freqs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "freq_df = pd.DataFrame(zip(corpus.corpus_meta, freqs))\n",
    "freq_df.columns = [\"doc\",\"freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>data/ocr/journal-article-10.2307_20467766.txt</td>\n",
       "      <td>4.803074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>data/ocr/journal-article-10.2307_25602021.txt</td>\n",
       "      <td>3.216296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>data/ocr/journal-article-10.2979_vic.2009.52.1...</td>\n",
       "      <td>3.556188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>data/ocr/journal-article-10.1525_ncl.2007.61.4...</td>\n",
       "      <td>3.384913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>data/ocr/journal-article-10.2307_43808677.txt</td>\n",
       "      <td>3.352892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12205</td>\n",
       "      <td>data/ocr/journal-article-10.2307_44085757.txt</td>\n",
       "      <td>3.503710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12212</td>\n",
       "      <td>data/ocr/journal-article-10.2307_25602028.txt</td>\n",
       "      <td>4.972032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12253</td>\n",
       "      <td>data/ocr/journal-article-10.2307_43973882.txt</td>\n",
       "      <td>3.802281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12255</td>\n",
       "      <td>data/ocr/journal-article-10.2307_40155580.txt</td>\n",
       "      <td>4.035684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12273</td>\n",
       "      <td>data/ocr/journal-article-10.2307_3736685.txt</td>\n",
       "      <td>4.957188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>763 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     doc      freq\n",
       "11         data/ocr/journal-article-10.2307_20467766.txt  4.803074\n",
       "25         data/ocr/journal-article-10.2307_25602021.txt  3.216296\n",
       "27     data/ocr/journal-article-10.2979_vic.2009.52.1...  3.556188\n",
       "39     data/ocr/journal-article-10.1525_ncl.2007.61.4...  3.384913\n",
       "77         data/ocr/journal-article-10.2307_43808677.txt  3.352892\n",
       "...                                                  ...       ...\n",
       "12205      data/ocr/journal-article-10.2307_44085757.txt  3.503710\n",
       "12212      data/ocr/journal-article-10.2307_25602028.txt  4.972032\n",
       "12253      data/ocr/journal-article-10.2307_43973882.txt  3.802281\n",
       "12255      data/ocr/journal-article-10.2307_40155580.txt  4.035684\n",
       "12273       data/ocr/journal-article-10.2307_3736685.txt  4.957188\n",
       "\n",
       "[763 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df[(freq_df.freq > 3) & (freq_df.freq < 5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
